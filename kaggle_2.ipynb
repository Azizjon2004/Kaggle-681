{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":29550,"sourceType":"datasetVersion","datasetId":23079}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport shutil\nimport pathlib\nimport itertools\nfrom PIL import Image\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\nfrom tensorflow.keras import regularizers\n\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom PIL import Image\nimport timm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint ('modules loaded')","metadata":{"execution":{"iopub.status.busy":"2024-12-13T08:48:44.858662Z","iopub.execute_input":"2024-12-13T08:48:44.859100Z","iopub.status.idle":"2024-12-13T08:48:44.867575Z","shell.execute_reply.started":"2024-12-13T08:48:44.859069Z","shell.execute_reply":"2024-12-13T08:48:44.866418Z"},"trusted":true},"outputs":[{"name":"stdout","text":"modules loaded\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"data_path = \"/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\" \n\nimages = []\nlabels = []\n\nfor subfolder in os.listdir(data_path):\n    \n    subfolder_path = os.path.join(data_path, subfolder)\n    if not os.path.isdir(subfolder_path):\n        continue\n  \n    for image_filename in os.listdir(subfolder_path):\n        image_path = os.path.join(subfolder_path, image_filename)\n        images.append(image_path)\n    \n        labels.append(subfolder)\n \ndata = pd.DataFrame({'image': images, 'label': labels})","metadata":{"execution":{"iopub.status.busy":"2024-12-13T08:23:52.206122Z","iopub.execute_input":"2024-12-13T08:23:52.206503Z","iopub.status.idle":"2024-12-13T08:23:52.392947Z","shell.execute_reply.started":"2024-12-13T08:23:52.206473Z","shell.execute_reply":"2024-12-13T08:23:52.391874Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-12T18:25:28.115482Z","iopub.execute_input":"2024-08-12T18:25:28.115809Z","iopub.status.idle":"2024-08-12T18:25:28.135175Z","shell.execute_reply.started":"2024-08-12T18:25:28.115778Z","shell.execute_reply":"2024-08-12T18:25:28.134321Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"strat = data['label']\ntrain_df, dummy_df = train_test_split(data,  train_size= 0.80, shuffle= True, random_state= 123, stratify= strat)\n\nstrat = dummy_df['label']\nvalid_df, test_df = train_test_split(dummy_df,  train_size= 0.5, shuffle= True, random_state= 123, stratify= strat)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T18:25:28.137092Z","iopub.execute_input":"2024-08-12T18:25:28.1374Z","iopub.status.idle":"2024-08-12T18:25:28.294948Z","shell.execute_reply.started":"2024-08-12T18:25:28.137376Z","shell.execute_reply":"2024-08-12T18:25:28.294147Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\nimg_size = (150, 150)\nchannels = 3\nimg_shape = (img_size[0], img_size[1], channels)\n\ntr_gen = ImageDataGenerator()\nts_gen = ImageDataGenerator()\n\ntrain_gen = tr_gen.flow_from_dataframe(train_df, x_col='image', y_col='label', target_size=img_size, class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n\nvalid_gen = ts_gen.flow_from_dataframe(valid_df, x_col='image', y_col='label', target_size=img_size, class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n\ntest_gen = ts_gen.flow_from_dataframe(test_df, x_col='image', y_col='label', target_size=img_size, class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T18:25:28.296076Z","iopub.execute_input":"2024-08-12T18:25:28.296354Z","iopub.status.idle":"2024-08-12T18:26:42.275181Z","shell.execute_reply.started":"2024-08-12T18:25:28.296329Z","shell.execute_reply":"2024-08-12T18:26:42.274418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using Compact Convolutional Transformer","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\n\ndef cct_model(input_shape=(150, 150, 3), num_classes=29):\n    inputs = layers.Input(shape=input_shape)\n\n    x = layers.Conv2D(64, (3, 3), padding='same', strides=(2, 2), activation='relu')(inputs)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.Conv2D(128, (3, 3), padding='same', strides=(2, 2), activation='relu')(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = layers.Flatten()(x)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Reshape((-1, 256))(x)\n\n    transformer_block = layers.MultiHeadAttention(num_heads=4, key_dim=64)\n    x = transformer_block(x, x)\n    x = layers.LayerNormalization(epsilon=1e-6)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.5)(x)\n\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\nmodel = cct_model(input_shape=(150, 150, 3), num_classes=29)\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T18:29:50.935517Z","iopub.execute_input":"2024-08-12T18:29:50.935978Z","iopub.status.idle":"2024-08-12T18:29:51.053899Z","shell.execute_reply.started":"2024-08-12T18:29:50.935947Z","shell.execute_reply":"2024-08-12T18:29:51.052951Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 5\n\nhistory = model.fit(\n    train_gen,\n    validation_data=valid_gen,\n    epochs=epochs,\n    verbose=1\n)\n\nloss, accuracy = model.evaluate(test_gen)\nprint(f'Test Loss: {loss}')\nprint(f'Test Accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T18:30:07.272509Z","iopub.execute_input":"2024-08-12T18:30:07.272883Z","iopub.status.idle":"2024-08-12T18:42:25.791306Z","shell.execute_reply.started":"2024-08-12T18:30:07.272857Z","shell.execute_reply":"2024-08-12T18:42:25.790428Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vision Transformer Base (ViT-B/16)","metadata":{}},{"cell_type":"code","source":"from transformers import ViTForImageClassification, ViTFeatureExtractor\nfrom PIL import Image\nimport torch\n\nmodel_name = 'google/vit-base-patch16-224-in21k'\nmodel = ViTForImageClassification.from_pretrained(model_name, num_labels=10)\nfeature_extractor = ViTFeatureExtractor()\n\ndef preprocess_image(image_path):\n    image = Image.open(image_path)\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n    return inputs\n\ndef predict(image_path):\n    inputs = preprocess_image(image_path)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class = torch.argmax(logits, dim=1)\n    return predicted_class\n\nimage_path = '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/A10.jpg'  \npredicted_class = predict(image_path)\nprint(f'Predicted class: {predicted_class.item()}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T19:27:30.249116Z","iopub.execute_input":"2024-08-12T19:27:30.249495Z","iopub.status.idle":"2024-08-12T19:27:31.080173Z","shell.execute_reply.started":"2024-08-12T19:27:30.249456Z","shell.execute_reply":"2024-08-12T19:27:31.07923Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vision Transformer Base(ViT-B/32)","metadata":{}},{"cell_type":"code","source":"model_name1 = 'google/vit-base-patch32-224-in21k'\nmodel1 = ViTForImageClassification.from_pretrained(model_name1, num_labels=10)\nfeature_extractor1 = ViTFeatureExtractor()\n\ndef preprocess_image(image_path):\n    image = Image.open(image_path)\n    inputs = feature_extractor1(images=image, return_tensors=\"pt\")\n    return inputs\n\ndef predict(image_path):\n    inputs = preprocess_image(image_path)\n    with torch.no_grad():\n        outputs = model1(**inputs)\n    logits = outputs.logits\n    predicted_class = torch.argmax(logits, dim=1)\n    return predicted_class\n\nimage_path = '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/A10.jpg'  \npredicted_class = predict(image_path)\nprint(f'Predicted class: {predicted_class.item()}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T19:31:10.767487Z","iopub.execute_input":"2024-08-12T19:31:10.768326Z","iopub.status.idle":"2024-08-12T19:31:11.477999Z","shell.execute_reply.started":"2024-08-12T19:31:10.768293Z","shell.execute_reply":"2024-08-12T19:31:11.476926Z"},"trusted":true},"outputs":[],"execution_count":null}]}